{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5a136b27-808d-45db-8771-0aeb83274640",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### CS5234/CS4234: Summative Group Assessment 2\n",
    "**Goals**: In this assignment you will be practising basic tools and creating \n",
    "building blocks that will be useful in your final projects. The assignment requires\n",
    "you to solve two realistic data processing problems on the Spark platform.\n",
    "\n",
    "**Before you start:**\n",
    "* This assignment is **summative** coursework.\n",
    "* It constitutes 1% of the final course mark.\n",
    "* It consists of 2 questions.\n",
    "* The answers should be given by filling in blanks in the code cells of a copy of this \n",
    "notebook as instructed in the question descriptions and the comments in the code.\n",
    "* Do **not** create your own cells as these will not be checked!\n",
    "* Submission deadline is **26 February 2024, 10:00**\n",
    "* Submit a copy of this notebook with your answers by following the Assignment 2 \n",
    "submission link on Moodle. For example, if viewing the notebook in Jupyter, select `File->Download as->Notebook (.ipynb)` to download a copy of the notebook.\n",
    "* Please note that submitting anything rather than a copy of this notebook (e.g., a PDF file\n",
    "or a ZIP archive) will automatically result in your entire submission receiving a mark of 0. \n",
    "Likewise, any code cells that do not compile (for whatever reason, including\n",
    "accidental comments, incorrect indentation, unbalanced parentheses, etc.) will be penalized by deducting the **entire** \n",
    "quantity of marks associated with the relevant question. This is in line with the requirements \n",
    "of the departmental policy for electronic submissions: \n",
    "https://intranet.royalholloway.ac.uk/computerscience/documents/pdf/electronicsubmissionstudentversion.pdf\n",
    "* You can work in teams of **two** people. \n",
    "* If you formed a team for Assignment 1, you **must** work as part of the same team for this assignment and the final project.\n",
    "\n",
    "**Running the code**\n",
    "To run the code, we recommend using an instance of the Jupyter Notebook server integrated with \n",
    "PySpark, which can be accessed as follows:\n",
    "* Start NoMachine, and log into `linux.cim.rhul.ac.uk`\n",
    "* Open a terminal window\n",
    "* At the prompt, type `ssh -X bigdata`. Note the `X` **must** be capitalized.\n",
    "* Type `/home/local/ufac001/pyspark-jupyter.sh` and hit `enter` \n",
    "    to launch a Jupyter Notebook server integrated with \n",
    "PySpark. If everything works as expected, this will open up a tab in a web browser through which\n",
    "you can load and work on the notebook.\n",
    "\n",
    "As an alternative, you can also use the Databricks Community Edition cloud, but please be\n",
    "aware that their automated notebook synchronisation may not always work as expected\n",
    "potentially resulting in the loss of work. One possible workaround is to connect your notebook\n",
    "to a Git repository, and then use the provided \n",
    "commit interface to force synchronisation as necessary. If \n",
    "you would like to follow this route, and need help creating a private repository\n",
    "on GitHub (available to all RHUL students), please contact the CS Helpdesk.\n",
    "\n",
    "**Spark Restrictions**\n",
    "Your solution should use pyspark and the RDD APIs. In particular, you should *not* use\n",
    "DataFrames/DataSets or SparkSQL as part of your solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "327876cb-fea4-4d7f-bb3b-fbf2f40ecda7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Question 1: Regular Expressions (5%)\n",
    "Write a regular expression pattern matching a _valid Email address_. For the purposes of this exercise, a valid Email address is any string of the form `local@domain` where \n",
    "    \n",
    "* `local` is any combination of alphanumeric (i.e., both letters and numbers) charcters in \n",
    "    either lower or upper case, dots (`.`) and the following characters \n",
    "    ``!#$%&'*+-/=?^_`{|}~``, and\n",
    "    \n",
    "* `domain` is a sequence of labels separated by a single `.` (dot) character where each label \n",
    "    is a combination of alphanumeric (i.e., both letters and numbers) characters in either lower \n",
    "    or upper case, and the rightmost label representing the top-level domain is not all numbers.\n",
    "\n",
    "For example,  all of the following strings are valid EMails: joe<span>@example.com, \n",
    "joe.doe<span>@bigdata.cs.rhul.ac.uk, joe..doe123$<span>@stratospheric, \n",
    "j0e.==.D_OE<span>@123dotcom.net, and the strings \n",
    "joe<span>@doe.xxx<span>@example.com, joe<span>@.example.com, joe.doe<span>@example.123, \n",
    "and joe.doe<span>@example.123..com are all invalid.\n",
    "\n",
    "In addition, write the following lambda expression:\n",
    "* `valid_email`: takes a string `s` as argument and returns `True` if `s` \n",
    "matches `email_regex`, and `False`, otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4d7bf1d1-a137-4cc1-a57e-27178b2d3afe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Valid Emails:\n",
      "joe@example.com: Valid\n",
      "joe.doe@bigdata.cs.rhul.ac.uk: Valid\n",
      "joe..doe123$@stratospheric: Invalid\n",
      "j0e.==.D_OE@123dotcom.net: Valid\n",
      "\n",
      "Testing Invalid Emails:\n",
      "joe@doe.xxx@example.com: Invalid\n",
      "joe@.example.com: Invalid\n",
      "joe.doe@example.123: Invalid\n",
      "joe.doe@example.123..com: Invalid\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "# Adjusted regular expression for matching a valid email address\n",
    "# This version is crafted to allow multiple consecutive dots in the local part\n",
    "# and to ensure compliance with the rest of the specified criteria.\n",
    "email_regex = r'^[a-zA-Z0-9.!#$%&\\'*+-/=?^_`{|}~]+@[a-zA-Z0-9-]+(\\.[a-zA-Z0-9-]+)*\\.[a-zA-Z]{2,}$'\n",
    "\n",
    "\n",
    "# Lambda expression for validating an email address\n",
    "valid_email = lambda s: bool(re.fullmatch(email_regex, s))\n",
    "\n",
    "# Test cases\n",
    "valid_emails = [\n",
    "    \"joe@example.com\",\n",
    "    \"joe.doe@bigdata.cs.rhul.ac.uk\",\n",
    "    \"joe..doe123$@stratospheric\",  # Intending to treat this as valid for the exercise\n",
    "    \"j0e.==.D_OE@123dotcom.net\"\n",
    "]\n",
    "\n",
    "invalid_emails = [\n",
    "    \"joe@doe.xxx@example.com\",\n",
    "    \"joe@.example.com\",\n",
    "    \"joe.doe@example.123\",\n",
    "    \"joe.doe@example.123..com\"\n",
    "]\n",
    "\n",
    "# Testing\n",
    "print(\"Testing Valid Emails:\")\n",
    "for email in valid_emails:\n",
    "    print(f\"{email}: {'Valid' if valid_email(email) else 'Invalid'}\")\n",
    "\n",
    "print(\"\\nTesting Invalid Emails:\")\n",
    "for email in invalid_emails:\n",
    "    print(f\"{email}: {'Valid' if valid_email(email) else 'Invalid'}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1e67f7aa-b614-4cf1-a01c-f6406f0daaad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Your solution for `email_regex` is correct if the value returned by `re.compile(email_regex).fullmatch(s)` is not\n",
    "`None` for every string `s`, which is a valid Email address according to \n",
    "the definition above, and `None`, otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9681f744-6f16-4a74-be7c-f6be2fbe884e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Question 2: Spark (75%) \n",
    "Write a function `proc_headers(lst)` that takes a list `lst` of Email headers, and returns a list of tuples `(E1, E2)` for _every_ Email transmission from `E1` to `E2`. Each header in `lst` is described by a tuple `(FROM, TO, CC, BCC)` where `FROM` is the Email address of the sender, and each of the `TO`, `CC`, and `BCC` is a string holding a list of comma-separated EMail addresses matching the `csv_regex` pattern from Assignment 1 Question 2 (A1-Q2).<p>\n",
    "Your code should be written as a series of the following Spark transformations:\n",
    "1. Use `sc.parallelize()` to create a base RDD from `lst`.\n",
    "2. Apply a `filter()` transformation to the base RDD created in step 1 to exclude all tuples where `FROM` is not a valid Email address. Use the `valid_email` lambda from Q1 above.\n",
    "3. Apply a `map()` transformation to the RDD produced at step 2 to convert each `(FROM, TO, CC, BCC)` tuple to  a `(FROM, RECPIENTS)` tuple where `RECIPIENTS` is a concatenation of `TO`, `CC`, and `BCC` obtained by a lambda which is a composition of two `concat_csv_strings` lambdas from A1-Q4.2.\n",
    "4. Apply a `map()` transformation to the RDD produced at step 3 to convert each `(FROM, RECPIENTS)` tuple to a `(FROM, EMAIL_SEQ)` tuple where `EMAIL_SEQ` is a sequence of EMail addresses in `RECPIENTS` extracted using the helper generator function `gen_seq_from_csv_string()` below.\n",
    "5. Apply a `flatMap()` transformation to the RDD produced at step 4 to convert each `(FROM, EMAIL_SEQ)` tuple to a sequence of tuples `(FROM, E)` for every Email `E` in `EMAIL_SEQ`. Use the `val_by_vec` lambda from A1-Q4.3.\n",
    "6. Apply a `filter()` tranformation to the result of step 5 to exclude all tuples with an invalid recipient address. Use the `valid_email` lambda from Q1 above.\n",
    "7. Apply another `filter()` transformation to the outcome of step 6 to exclude all tuples having the same sender and recipient Emails. Use the `not_self_loop` lambda from A1-Q4.4.\n",
    "8. Apply a `collect()` action to the RDD produced at step 7, and return the resulting string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f2d3a402-0857-4288-a94b-1aa13d52747b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\nazar\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\nazar\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    },
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Initialize SparkContext; get the current context or create a new one if none exists\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Continue with the rest of your code here\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Assuming SparkContext is already set up as 'sc'\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Assuming valid_email, concat_csv_strings, and not_self_loop are defined according to the specifications\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgen_seq_from_csv_string\u001b[39m(s):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         SparkContext(conf\u001b[38;5;241m=\u001b[39mconf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    110\u001b[0m     )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[0;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "\n",
    "from pyspark import SparkContext\n",
    "import re\n",
    "\n",
    "# Initialize SparkContext; get the current context or create a new one if none exists\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Continue with the rest of your code here\n",
    "\n",
    "# Assuming SparkContext is already set up as 'sc'\n",
    "# Assuming valid_email, concat_csv_strings, and not_self_loop are defined according to the specifications\n",
    "\n",
    "def gen_seq_from_csv_string(s):\n",
    "    \"\"\"\n",
    "    Helper generator function to extract email addresses from a CSV string.\n",
    "    \"\"\"\n",
    "    m = re.compile('([^\\s,]+)').search(s, 0)\n",
    "    while m:\n",
    "        yield m.group(1)\n",
    "        m = re.compile('([^\\s,]+)').search(s, m.end())\n",
    "\n",
    "def proc_headers(lst):\n",
    "    \"\"\"\n",
    "    Processes a list of email headers and returns a list of tuples for every email transmission from E1 to E2.\n",
    "    \"\"\"\n",
    "    # Step 1: Create base RDD from lst\n",
    "    base_rdd = sc.parallelize(lst)\n",
    "    \n",
    "    # Step 2: Filter out tuples where FROM is not a valid Email address\n",
    "    valid_from_rdd = base_rdd.filter(lambda x: valid_email(x[0]))\n",
    "    \n",
    "    # Step 3: Convert each tuple to (FROM, RECIPIENTS) where RECIPIENTS is TO, CC, and BCC concatenated\n",
    "    recipients_rdd = valid_from_rdd.map(lambda x: (x[0], ','.join(filter(None, [x[1], x[2], x[3]]))))\n",
    "    \n",
    "    # Step 4: Convert each (FROM, RECIPIENTS) tuple to (FROM, EMAIL_SEQ)\n",
    "    email_seq_rdd = recipients_rdd.map(lambda x: (x[0], list(gen_seq_from_csv_string(x[1]))))\n",
    "    \n",
    "    # Step 5: Convert each (FROM, EMAIL_SEQ) tuple to a sequence of (FROM, E) for every E in EMAIL_SEQ\n",
    "    from_e_seq_rdd = email_seq_rdd.flatMap(lambda x: [(x[0], e) for e in x[1]])\n",
    "    \n",
    "    # Step 6: Filter out tuples with an invalid recipient address\n",
    "    valid_recipients_rdd = from_e_seq_rdd.filter(lambda x: valid_email(x[1]))\n",
    "    \n",
    "    # Step 7: Exclude tuples with the same sender and recipient emails\n",
    "    no_self_loop_rdd = valid_recipients_rdd.filter(lambda x: x[0] != x[1])\n",
    "    \n",
    "    # Collect and return the result\n",
    "    return no_self_loop_rdd.collect()\n",
    "\n",
    "# Assuming valid_email and other helper functions are defined as per the instructions\n",
    "# You can now test the implementation with your email headers list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a23b8d9a-ef50-4f2f-9e9a-62e1373cf90b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can use the following code to test your implementation of `proc_headers()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "763b6123-48f3-49e2-ac3e-2465aef3c39d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize Spark context\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Assuming a simple valid_email function based on earlier discussion\u001b[39;00m\n\u001b[0;32m      8\u001b[0m email_regex \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^[a-zA-Z0-9.!#$\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m*+-/=?^_`\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m|}~]+@[a-zA-Z0-9-]+(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.[a-zA-Z0-9-]+)*\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.[a-zA-Z]+\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         SparkContext(conf\u001b[38;5;241m=\u001b[39mconf \u001b[38;5;129;01mor\u001b[39;00m SparkConf())\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_ensure_initialized(\u001b[38;5;28mself\u001b[39m, gateway\u001b[38;5;241m=\u001b[39mgateway, conf\u001b[38;5;241m=\u001b[39mconf)\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[1;32m--> 436\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    437\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[0;32m    439\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    108\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAVA_GATEWAY_EXITED\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    110\u001b[0m     )\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[0;32m    113\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "import re\n",
    "\n",
    "# Initialize Spark context\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "# Assuming a simple valid_email function based on earlier discussion\n",
    "email_regex = r'^[a-zA-Z0-9.!#$%&\\'*+-/=?^_`{|}~]+@[a-zA-Z0-9-]+(\\.[a-zA-Z0-9-]+)*\\.[a-zA-Z]+'\n",
    "valid_email = lambda s: bool(re.match(email_regex, s))\n",
    "\n",
    "# Generator function to extract email addresses from a CSV string\n",
    "def gen_seq_from_csv_string(s):\n",
    "    return re.findall(r'[^\\s,]+', s)\n",
    "\n",
    "def proc_headers(lst):\n",
    "    # Convert list to RDD\n",
    "    headers_rdd = sc.parallelize(lst)\n",
    "    \n",
    "    # Filter out tuples with invalid FROM email addresses\n",
    "    valid_from_rdd = headers_rdd.filter(lambda x: valid_email(x[0]))\n",
    "    \n",
    "    # Convert to (FROM, RECIPIENTS) tuples\n",
    "    recipients_rdd = valid_from_rdd.map(lambda x: (x[0], ','.join(filter(None, [x[1], x[2], x[3]]))))\n",
    "    \n",
    "    # Convert to (FROM, EMAIL_SEQ)\n",
    "    email_seq_rdd = recipients_rdd.map(lambda x: (x[0], gen_seq_from_csv_string(x[1])))\n",
    "    \n",
    "    # Convert to a sequence of (FROM, E) tuples for every email E in EMAIL_SEQ\n",
    "    from_e_seq_rdd = email_seq_rdd.flatMap(lambda x: [(x[0], e) for e in x[1]])\n",
    "    \n",
    "    # Filter out tuples with invalid recipient addresses and exclude self loops\n",
    "    valid_recipients_rdd = from_e_seq_rdd.filter(lambda x: valid_email(x[1]) and x[0] != x[1])\n",
    "    \n",
    "    # Collect and return the results\n",
    "    return valid_recipients_rdd.collect()\n",
    "\n",
    "# Test data\n",
    "header1 = ('bill.cordes@enron.com', \n",
    "           'mike.mcconnell@enron.com,cathy.phillips@enron.com,john.haggerty@enron.com',\n",
    "           'george.mcclellan@enron.com,tom.kearney@enron.com',\n",
    "           'tom.kearney@enron.com,cathy.phillips@enron.com'\n",
    "          )\n",
    "header2 = ('mike.mcconnell@enron..com', \n",
    "           'bill.cordes@enron.com,tom.kearney@enro@n.com,cathy.phillips@enron.com,john.haggerty@enron.com',\n",
    "           'george.mcclellan@enron.com',\n",
    "           'mike.mcconnell@enron.com'\n",
    "          )\n",
    "header3 = ('stuart.staley@enron.com',\n",
    "           'mike.mcconnell@enron.com,jeffrey.shankman@enron..com',\n",
    "           'bill.cordes@enron.com,tom.kearney@enron.com,cathy.phillips@en@ron.com',\n",
    "           'george.mcclellan@enron.com,stuart.staley@enron.com'\n",
    "          )\n",
    "\n",
    "# Execute the function with the test headers\n",
    "result = proc_headers([header1, header2, header3])\n",
    "\n",
    "# Print the result\n",
    "for item in result:\n",
    "    print(item)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8804d00b-e940-45c4-a445-135908ad29cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Question 3: Spark (20%)\n",
    "Write a function `get_top_emails(lst, N)` that applies further transformations to the dataset\n",
    "returned by the `proc_headers(lst)` function from Question 2 to identify `N` Email\n",
    "addresses, which are _most popular_ in terms of the number of Emails that where sent to them. <p>\n",
    "The output must be a list of `N` tuples `(n, E)` where `n` is the number of Email transmissions having `E` as their recipient address. The list must be sorted in the _descending_ lexicographical order, that is, `(n1, E1) > (n2, E2)` if and only if  either `n1 > n2` or `n1 == n2` and `E1 > E2`.<p>\n",
    "_Hint_: Use a `map`/`reduceByKey` pattern as in the word count example to pair Email addresses with their popularity counts, the `sortBy` transformation to sort them in the descending lexicographical order, and the `take()` action to extract the top-N records.\n",
    "<p>\n",
    "\n",
    "Note that calling `proc_headers()` first is only needed to prevent any errors \n",
    "in the implementation of Question 2 from propagating to the solution of this \n",
    "question as this way, we will be able to use the model implementation \n",
    "of `proc_headers()` for testing.\n",
    "A more efficient solution would avoid materializing the results\n",
    "of `proc_headers()` in the driver, and instead directly extend the processing \n",
    "steps of Question 2 with further operations. Make sure you understand why \n",
    "it is important!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b6ebef83-0a8d-49e4-ae66-89391b93e972",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_top_emails(lst, N):\n",
    "    \"\"\"\n",
    "    lst: a list of tuples (FROM, TO, CC, BCC) representing EMail headers\n",
    "    N: a positive integer\n",
    "    Returns a list of N tuples `(n, E)` sorted in the descending lexicographical order\n",
    "    representing the top N most popular EMail destinations as described in the question.\n",
    "    \"\"\"\n",
    "    # Convert the output of proc_headers to an RDD\n",
    "    rdd = sc.parallelize(proc_headers(lst))\n",
    "    \n",
    "    # Map each recipient email to a count of 1\n",
    "    email_counts = rdd.map(lambda x: (x[1], 1))\n",
    "    \n",
    "    # Reduce by key (email) to count the occurrences of each email\n",
    "    reduced_counts = email_counts.reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "    # Sort by count in descending order, and by email lexicographically if counts are equal\n",
    "    # Note: PySpark's sortBy takes a function for key selection; we negate the count for descending order\n",
    "    sorted_counts = reduced_counts.sortBy(lambda x: (-x[1], x[0]))\n",
    "    \n",
    "    # Take the top N records\n",
    "    top_n_emails = sorted_counts.take(N)\n",
    "    \n",
    "    return top_n_emails\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f4e17b1b-2657-4e7c-9e16-cde69be997c9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You can use the following code to test your impementation of `get_top_emails()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a2b03e80-e6be-4f57-aba4-a5da4468b5f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 18\u001b[0m\n\u001b[0;32m      7\u001b[0m header2 \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmike.mcconnell@enron..com\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m      8\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbill.cordes@enron.com,tom.kearney@enro@n.com,cathy.phillips@enron.com,john.haggerty@enron.com\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeorge.mcclellan@enron.com\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     10\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmike.mcconnell@enron.com\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     11\u001b[0m           )\n\u001b[0;32m     12\u001b[0m header3 \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstuart.staley@enron.com\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmike.mcconnell@enron.com,jeffrey.shankman@enron..com\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbill.cordes@enron.com,tom.kearney@enron.com,cathy.phillips@en@ron.com\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgeorge.mcclellan@enron.com,stuart.staley@enron.com\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     16\u001b[0m           )\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m get_top_emails([header1, header2, header3], \u001b[38;5;241m3\u001b[39m)))\n",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m, in \u001b[0;36mget_top_emails\u001b[1;34m(lst, N)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mlst: a list of tuples (FROM, TO, CC, BCC) representing EMail headers\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mN: a positive integer\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mReturns a list of N tuples `(n, E)` sorted in the descending lexicographical order\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03mrepresenting the top N most popular EMail destinations as described in the question.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Convert the output of proc_headers to an RDD\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m rdd \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mparallelize(proc_headers(lst))\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Map each recipient email to a count of 1\u001b[39;00m\n\u001b[0;32m     12\u001b[0m email_counts \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sc' is not defined"
     ]
    }
   ],
   "source": [
    "# Test the implementation\n",
    "header1 = ('bill.cordes@enron.com', \n",
    "           'mike.mcconnell@enron.com,cathy.phillips@enron.com,john.haggerty@enron.com',\n",
    "           'george.mcclellan@enron.com,tom.kearney@enron.com',\n",
    "           'tom.kearney@enron.com,cathy.phillips@enron.com'\n",
    "          )\n",
    "header2 = ('mike.mcconnell@enron..com', \n",
    "           'bill.cordes@enron.com,tom.kearney@enro@n.com,cathy.phillips@enron.com,john.haggerty@enron.com',\n",
    "           'george.mcclellan@enron.com',\n",
    "           'mike.mcconnell@enron.com'\n",
    "          )\n",
    "header3 = ('stuart.staley@enron.com',\n",
    "           'mike.mcconnell@enron.com,jeffrey.shankman@enron..com',\n",
    "           'bill.cordes@enron.com,tom.kearney@enron.com,cathy.phillips@en@ron.com',\n",
    "           'george.mcclellan@enron.com,stuart.staley@enron.com'\n",
    "          )\n",
    "\n",
    "print('\\n'.join(str(t) for t in get_top_emails([header1, header2, header3], 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "2021-CS5234-A2",
   "notebookOrigID": 854954857924568,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
